### Introduction

1. 在100B到1000B参数规模时。由于LLM的成功应用和效能，以及模型浮点运算(FLOPS)利用率(MFU)的评估问题，需要对模型大小进行更有效的扩展。
2. 当前的人工智能(AI)加速器在模型训练过程中的利用率可能低至50%或更低，因此有必要显著提高MFU以使模型大小在类似当前系统的架构上提高10倍（10万亿参数）或更高。

### Background

软硬件协同设计(Co-design) - 通过协同优化软硬件架构来提升LLM的训练效率。

GPU集群(GPU Clusters) - 基于类似NVIDIA DGX/HGX的多GPU节点集群的分布式训练平台。

内存优化(Memory Optimization) - 通过张量卸载等方式优化内存使用,支持更大模型。

并行策略(Parallelism) - DP, TP, PP等方式分布训练计算。

性能建模(Performance Modeling) - 建立快速的分析性能模型来指导软硬件的协同设计。

### 总体目标

本文旨在寻找在8个互连GPU的大型系统（类似于NVIDIA DGX和HGX）上训练多万亿参数模型的性能瓶颈和系统限制，同时结合算子分发和并行等确定大模型参数量和尺寸的理想关系。

### 方法:
a. 理论背景：

使用了一种名为Calculon的开源快速分析性能模型来估计给定LLM，系统配置和软件执行策略的时间和资源使用情况。

b. 技术方法：

为了验证其模型的精度，作者们将Calculon与在NVIDIA的A100-based Selene超级计算机上实际运行的Megatron LLM进行了比较。

另外，作者们使用tensor offloading技术来增加可训练LLM的大小。他们的发现表明，当前的H100 GPU，配备80 GiB的HBM和512 GiB的tensor offloading能力，可以扩展到11T参数的LLM；而达到128T参数需要120 GiB的HBM和2 TiB的offloading内存，从而实现75%+ MFU。

### 结果：
a. 详细的实验设置：

在实验中，作者们考虑了系统大小，模型大小，内存容量，带宽和NVLink域大小等因素的变化。每个系统，都选择了一个执行策略，该策略考虑了多种最新的软件优化，并选择了性能最佳的一个。

b. 详细的实验结果：

作者们发现，训练百万亿参数的LLM是可行的，但每个GPU需要高达1 TiB的二级内存池，并且带宽为100 GB/s双向。此外，他们发现，10T模型以上的扩展需要更多的一级内存，其中HBM大小随模型大小而缩放。

总的来说，作者们发现，实现高性能和效率将至关重要，需要对LLM，软件和硬件进行联合设计。